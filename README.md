# Team Eclipse

# Proposal for the Birth of Wise AI

**Balancing Long-Term Vision and Practical Early Implementation for Responsible AI Innovation**

## 1. Introduction: The Foundation for Responsible AI Innovation

Artificial intelligence (AI) brings unprecedented innovation and opportunity to humanity. However, the potential risks—such as malfunction, bias, and misuse—are fundamentally different in scale and nature from those of previous technologies. While many countries are in the early stages of AI risk management, there remains a significant gap compared to the rigorous safety management frameworks established in high-risk industries like aviation, nuclear energy, and pharmaceuticals.

This proposal argues for the application of stricter and more comprehensive safety standards to AI systems, taking into account their unique risk characteristics. It also presents practical, phased early implementation strategies to achieve this long-term vision. The ultimate goal is to foster sustainable and responsible growth in the AI industry through a paradigm shift:

**Safe AI is the best competitive advantage.**

## 2. Unique Risk Characteristics of AI Systems and the Need for Risk Management

### 2.1. Unprecedented Scale of Impact

- Aviation accidents: Hundreds affected, regional/national impact
- Nuclear accidents: Tens of thousands affected, national/cross-border impact
- Pharmaceutical accidents: Thousands affected, specific group impact
- AI incidents: Potentially hundreds of millions affected simultaneously (e.g., algorithmic errors, mass data breaches, cyberattacks), paralysis of global systems, threats to democracy

### 2.2. Irreversible and Permanent Damage

- Data breaches: Permanent harm, ongoing risk of misuse
- Algorithmic bias: Entrenched social discrimination, erosion of fairness
- Deepfakes and misinformation: Social chaos, loss of trust, threats to democratic institutions

### 2.3. Extreme Unpredictability

- Emergent behaviors: Unforeseen errors or actions due to subtle data changes or complex internal mechanisms
- Black-box nature: Difficulty in tracing causes and preventing recurrence
- Complex interactions: Cascading failures between AI systems, leading to uncontrollable situations

## 3. Comparison of Major AI Risk Management Frameworks and Traditional Safety Laws

| Category                | AI Laws (High-Risk AI)         | Aviation Safety Law (Aircraft)       | Nuclear Safety Law (Reactors)       | Pharmaceutical Safety Law (Drugs)        |
|-------------------------|-------------------------------|--------------------------------------|-------------------------------------|------------------------------------------|
| Maximum Sanction        | Up to 7% of global turnover or equivalent fines    | Suspension of operation, license revocation, criminal penalties | Reactor shutdown, license revocation, criminal penalties | Market withdrawal, license revocation, criminal penalties |
| Emergency Powers        | No immediate effect (recommendations) | Immediate suspension of operations         | Immediate shutdown                  | Immediate sales suspension, mandatory recall |
| Pre-market Evaluation   | Self-assessment or certification for high-risk AI | Mandatory for all aircraft design/manufacture | Mandatory for all reactor construction/operation | Mandatory preclinical/clinical trials for all drugs |
| Ongoing Monitoring      | Voluntary reporting, post-hoc investigation | 24/7 control, regular inspections   | Real-time monitoring, constant oversight | Post-market surveillance, adverse event monitoring |
| Operator Qualifications | No explicit requirements (training recommended) | Pilot license, operator qualification | Reactor operator license, institution qualification | Professional qualifications (doctor, pharmacist) |

**Analysis:**  
Current AI laws apply a lower level of risk management rigor, immediate control, pre-market validation, ongoing oversight, and operator qualification compared to traditional safety laws.  
Traditional industries employ a "life cycle management" principle with multi-layered safety nets, while AI risk management still shows critical gaps.

## 4. Strengthening AI System Safety Management: Long-Term Vision and Early Implementation

### 4.1. Long-Term Vision: Comprehensive and Rigorous AI Safety Framework

- **Pre-market Evaluation and Approval:**  
  - Mandatory "AI clinical trials" for high-risk or advanced AI systems (Phase 1: simulation, Phase 2: limited real-world use, Phase 3: large-scale validation)
  - Stronger responsibility for safety validation by developers/manufacturers

- **Real-Time Monitoring and Emergency Response:**  
  - Establishment of 24/7 AI safety control and emergency response centers
  - Mandatory real-time performance monitoring systems

- **Emergency Powers and Recall/Shutdown Systems:**  
  - Immediate service suspension orders for critical safety issues
  - Mandatory remote emergency shutdown ("kill switch") functionality

- **Operator Qualification Requirements:**  
  - Licensing/certification for high-risk AI developers/operators
  - Clear assignment of responsibility and effective enforcement mechanisms

**Practical Scenario Example:**  
- Case: A national AI safety monitoring center, modeled after air traffic control towers, enables real-time response to AI system anomalies and rapid shutdown when necessary.

### 4.2. Early Implementation: Incentive-Based Voluntary Participation and Transparency

- **Objective:** Encourage companies developing/operating high-risk AI systems to self-assess and improve safety, building public trust.

- **Scope:**  
  - High-risk AI includes generative AI systems with significant social impact (e.g., over 1 million monthly active users).

- **Assessment Criteria:**  
  - Risk management framework  
  - Data governance  
  - Performance and robustness  
  - Transparency and explainability  
  - Human oversight and control  
  - Security

- **Operation:**  
  - Development and distribution of "High-Risk AI Safety Self-Assessment Guidelines"
  - Companies meeting standards may apply for an "AI Safety Certification Mark"
  - Initial focus on simplified certification based on self-assessment reports

- **Incentives for Participation:**  
  - Tax benefits (corporate tax relief, R&D credits)
  - Preferential treatment in public procurement and government projects
  - Priority access to public funding and support
  - Enhanced brand value (government-backed promotion)
  - Regulatory sandbox advantages

**Real-World Case Studies:**  
- Tesla: After early accidents with autonomous vehicles, Tesla invested heavily in safety systems, improving Full Self-Driving (FSD) and increasing market value.
- IBM Watson Health: Achieved market dominance in healthcare AI through rigorous safety and reliability certification, now a prerequisite for adoption in hospitals and health systems.

## 5. Anticipated Challenges and Solutions

### 5.1. Industry Resistance to Risk Management

- **Paradigm shift:** "Safety is competitiveness"
- **Solution:** Emphasize success stories in aviation, pharmaceuticals, and AI healthcare to show that safety investment enhances competitiveness and revenue. Incentive-based voluntary participation to minimize resistance.

### 5.2. Disparities in Risk Management Approaches Between Countries

- **Solution:**  
  - Establish minimum international standards through global cooperation.
  - Benchmark successful models from sectors like aviation and nuclear energy.
  - Start with partial agreements on high-risk AI and expand gradually.

### 5.3. Intrinsic Complexity of AI Systems

- **Solution:**  
  - Invest in and mandate AI safety technologies (explainability, robustness, bias mitigation).
  - Promote universal adoption of remote shutdown and data governance transparency.

## 6. Stepwise Roadmap: Flexible and Incremental Approach

| Phase        | Key Actions                                                                                      |
|--------------|-------------------------------------------------------------------------------------------------|
| **Short-Term (1 year)** | Develop/distribute "High-Risk AI Safety Self-Assessment Guidelines"Launch AI safety certification and incentivesMandate AI-generated content disclosureBegin training for high-risk AI developers/operators |
| **Mid-Term (3 years)** | Expand participation, analyze effectivenessResearch/pilot AI safety control and emergency centersIncrease support for AI safety R&D and standardizationEncourage summary disclosure of high-risk AI safety assessments |
| **Long-Term (5 years)** | Consider mandatory external audits for high-risk AIReview universal pre-market approval and operator qualificationLead/participate in international AI safety agreements and mandatory technical standards |

## 7. Conclusion and Policy Recommendations

AI system risks already surpass those of traditional industries, yet risk management remains insufficient.

A paradigm shift is urgently needed.

- Old: "AI is new technology, so deregulation is needed for innovation."
- New: "AI is high-risk technology, so stronger risk management is essential for true innovation."

**Immediate Action Items:**

- Define a long-term vision and roadmap for comprehensive AI safety management
- Implement early-stage strategies: self-assessment, certification, incentives, AI-generated content disclosure
- Accelerate AI safety technology development, standardization, and legal requirements
- Lead international cooperation and prevent regulatory arbitrage
- Build social consensus and a culture of safety

AI's potential must be recognized, but its risks must not be underestimated. Applying higher standards than traditional safety laws is both rational and essential for the sustainable, responsible growth of the AI industry. Now is the decisive moment to shift the AI safety paradigm and put it into action.

## Appendices & Additional Notes

### Glossary of Terms

A comprehensive glossary defining key terms such as "High-Risk AI", "Risk Management Framework", "Explainability", etc., is provided for policymakers and practitioners.

### Social Impact Assessment (SIA)

A recommended framework for evaluating the social and ethical impacts of AI systems—including qualitative and quantitative methodologies, stakeholder analysis, and risk mapping—is included as a supplementary section.

### Practical Application Scenarios & Case Studies

- Tesla Autopilot: Demonstrates the business value of investing in AI safety after early failures.
- IBM Watson Health: Shows the market advantage of rigorous safety certification in healthcare AI.
- Scenario Example: Establishing a national AI safety monitoring center modeled after aviation control towers for real-time anomaly detection and emergency intervention.

These enhancements provide greater practical realism and user-friendliness, making the policy proposal more actionable for government officials, industry leaders, and AI safety professionals worldwide.
